{
  "timestamp": "20250902_130716",
  "dockerfile_path": "envgym/envgym.dockerfile",
  "image_name": "envgym_test_1756818428",
  "build": {
    "success": true,
    "stdout": "",
    "stderr": "#0 building with \"default\" instance using docker driver\n\n#1 [internal] load build definition from envgym.dockerfile\n#1 transferring dockerfile: 9.55kB done\n#1 DONE 0.0s\n\n#2 [internal] load metadata for docker.io/library/python:3.9-slim\n#2 DONE 0.2s\n\n#3 [internal] load .dockerignore\n#3 transferring context: 2B done\n#3 DONE 0.0s\n\n#4 [ 1/20] FROM docker.io/library/python:3.9-slim@sha256:914169c7c8398b1b90c0b0ff921c8027445e39d7c25dc440337e56ce0f2566e6\n#4 DONE 0.0s\n\n#5 [ 9/20] RUN /home/cc/EnvGym/data/TabPFN/venv/bin/pip install --upgrade pip setuptools wheel\n#5 CACHED\n\n#6 [10/20] RUN /home/cc/EnvGym/data/TabPFN/venv/bin/pip install uv\n#6 CACHED\n\n#7 [ 7/20] RUN git submodule update --init --recursive\n#7 CACHED\n\n#8 [13/20] RUN /home/cc/EnvGym/data/TabPFN/venv/bin/pip install     \"scikit-learn>=1.2.0,<1.7\"     \"pandas>=1.4.0,<3\"     \"scipy>=1.11.1,<2\"     \"einops>=0.2.0,<0.9\"     \"huggingface-hub\"     \"pydantic>=2.8.0\"     \"pydantic-settings>=2.0.0\"     \"python-dotenv\"     \"typing-extensions\"\n#8 CACHED\n\n#9 [12/20] RUN /home/cc/EnvGym/data/TabPFN/venv/bin/pip install --no-deps .\n#9 CACHED\n\n#10 [ 5/20] RUN git clone https://github.com/priorlabs/tabpfn.git --depth 1 TabPFN\n#10 CACHED\n\n#11 [11/20] RUN /home/cc/EnvGym/data/TabPFN/venv/bin/pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n#11 CACHED\n\n#12 [ 4/20] WORKDIR /home/cc/EnvGym/data\n#12 CACHED\n\n#13 [ 2/20] RUN apt-get update && apt-get install -y     git     curl     build-essential     && rm -rf /var/lib/apt/lists/*\n#13 CACHED\n\n#14 [ 6/20] WORKDIR /home/cc/EnvGym/data/TabPFN\n#14 CACHED\n\n#15 [16/20] RUN echo '# TabPFN Settings\\nTABPFN_MODEL_CACHE_DIR=/home/cc/EnvGym/data/TabPFN/models\\nTABPFN_ALLOW_CPU_LARGE_DATASET=true\\nTABPFN_EXCLUDE_DEVICES=cuda,mps\\n\\nPYTORCH_CUDA_ALLOC_CONF=\\nCUDA_VISIBLE_DEVICES=\\n\\nFORCE_CONSISTENCY_TESTS=0\\nCI=false' > .env\n#15 CACHED\n\n#16 [ 3/20] RUN curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg     && chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg     && echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null     && apt-get update     && apt-get install gh -y     && rm -rf /var/lib/apt/lists/*\n#16 CACHED\n\n#17 [18/20] RUN mkdir -p scripts &&     echo '#!/usr/bin/env python3' > scripts/generate_dependencies.py &&     echo 'import sys' >> scripts/generate_dependencies.py &&     echo 'import re' >> scripts/generate_dependencies.py &&     echo 'from pathlib import Path' >> scripts/generate_dependencies.py &&     echo '' >> scripts/generate_dependencies.py &&     echo 'def parse_pyproject_toml():' >> scripts/generate_dependencies.py &&     echo '    \"\"\"Parse pyproject.toml and extract dependencies.\"\"\"' >> scripts/generate_dependencies.py &&     echo '    pyproject_path = Path(\"pyproject.toml\")' >> scripts/generate_dependencies.py &&     echo '    if not pyproject_path.exists():' >> scripts/generate_dependencies.py &&     echo '        print(\"pyproject.toml not found\")' >> scripts/generate_dependencies.py &&     echo '        sys.exit(1)' >> scripts/generate_dependencies.py &&     echo '    ' >> scripts/generate_dependencies.py &&     echo '    content = pyproject_path.read_text()' >> scripts/generate_dependencies.py &&     echo '    ' >> scripts/generate_dependencies.py &&     echo '    # Extract dependencies section' >> scripts/generate_dependencies.py &&     echo '    deps_match = re.search(r\"dependencies = \\[(.*?)\\]\", content, re.DOTALL)' >> scripts/generate_dependencies.py &&     echo '    if not deps_match:' >> scripts/generate_dependencies.py &&     echo '        print(\"No dependencies found\")' >> scripts/generate_dependencies.py &&     echo '        return []' >> scripts/generate_dependencies.py &&     echo '    ' >> scripts/generate_dependencies.py &&     echo '    deps_text = deps_match.group(1)' >> scripts/generate_dependencies.py &&     echo '    deps = re.findall(r'\"'\"'\"([^\"]+)\"'\"'\"', deps_text)' >> scripts/generate_dependencies.py &&     echo '    return deps' >> scripts/generate_dependencies.py &&     echo '' >> scripts/generate_dependencies.py &&     echo 'def generate_minimum_requirements(deps):' >> scripts/generate_dependencies.py &&     echo '    \"\"\"Generate requirements with minimum versions.\"\"\"' >> scripts/generate_dependencies.py &&     echo '    min_reqs = []' >> scripts/generate_dependencies.py &&     echo '    for dep in deps:' >> scripts/generate_dependencies.py &&     echo '        if \">=\" in dep:' >> scripts/generate_dependencies.py &&     echo '            # Keep minimum version' >> scripts/generate_dependencies.py &&     echo '            min_reqs.append(dep.split(\",\")[0])' >> scripts/generate_dependencies.py &&     echo '        else:' >> scripts/generate_dependencies.py &&     echo '            min_reqs.append(dep)' >> scripts/generate_dependencies.py &&     echo '    return min_reqs' >> scripts/generate_dependencies.py &&     echo '' >> scripts/generate_dependencies.py &&     echo 'def generate_maximum_requirements(deps):' >> scripts/generate_dependencies.py &&     echo '    \"\"\"Generate requirements with maximum versions.\"\"\"' >> scripts/generate_dependencies.py &&     echo '    max_reqs = []' >> scripts/generate_dependencies.py &&     echo '    for dep in deps:' >> scripts/generate_dependencies.py &&     echo '        if \"<\" in dep:' >> scripts/generate_dependencies.py &&     echo '            # Extract package name and max version' >> scripts/generate_dependencies.py &&     echo '            parts = dep.split(\">\")' >> scripts/generate_dependencies.py &&     echo '            if len(parts) > 1:' >> scripts/generate_dependencies.py &&     echo '                pkg_name = parts[0]' >> scripts/generate_dependencies.py &&     echo '                max_part = dep.split(\"<\")[-1]' >> scripts/generate_dependencies.py &&     echo '                max_version = max_part.strip()' >> scripts/generate_dependencies.py &&     echo '                # Convert < to ==' >> scripts/generate_dependencies.py &&     echo '                if max_version:' >> scripts/generate_dependencies.py &&     echo '                    max_reqs.append(f\"{pkg_name}=={max_version}\")' >> scripts/generate_dependencies.py &&     echo '                else:' >> scripts/generate_dependencies.py &&     echo '                    max_reqs.append(dep)' >> scripts/generate_dependencies.py &&     echo '            else:' >> scripts/generate_dependencies.py &&     echo '                max_reqs.append(dep)' >> scripts/generate_dependencies.py &&     echo '        else:' >> scripts/generate_dependencies.py &&     echo '            max_reqs.append(dep)' >> scripts/generate_dependencies.py &&     echo '    return max_reqs' >> scripts/generate_dependencies.py &&     echo '' >> scripts/generate_dependencies.py &&     echo 'def main():' >> scripts/generate_dependencies.py &&     echo '    if len(sys.argv) != 2 or sys.argv[1] not in [\"minimum\", \"maximum\"]:' >> scripts/generate_dependencies.py &&     echo '        print(\"Usage: python generate_dependencies.py [minimum|maximum]\")' >> scripts/generate_dependencies.py &&     echo '        sys.exit(1)' >> scripts/generate_dependencies.py &&     echo '    ' >> scripts/generate_dependencies.py &&     echo '    mode = sys.argv[1]' >> scripts/generate_dependencies.py &&     echo '    deps = parse_pyproject_toml()' >> scripts/generate_dependencies.py &&     echo '    ' >> scripts/generate_dependencies.py &&     echo '    if mode == \"minimum\":' >> scripts/generate_dependencies.py &&     echo '        reqs = generate_minimum_requirements(deps)' >> scripts/generate_dependencies.py &&     echo '        output_file = \"requirements-minimum.txt\"' >> scripts/generate_dependencies.py &&     echo '    else:' >> scripts/generate_dependencies.py &&     echo '        reqs = generate_maximum_requirements(deps)' >> scripts/generate_dependencies.py &&     echo '        output_file = \"requirements-maximum.txt\"' >> scripts/generate_dependencies.py &&     echo '    ' >> scripts/generate_dependencies.py &&     echo '    with open(output_file, \"w\") as f:' >> scripts/generate_dependencies.py &&     echo '        for req in reqs:' >> scripts/generate_dependencies.py &&     echo '            f.write(req + \"\\\\n\")' >> scripts/generate_dependencies.py &&     echo '    ' >> scripts/generate_dependencies.py &&     echo '    print(f\"Generated {output_file}\")' >> scripts/generate_dependencies.py &&     echo '' >> scripts/generate_dependencies.py &&     echo 'if __name__ == \"__main__\":' >> scripts/generate_dependencies.py &&     echo '    main()' >> scripts/generate_dependencies.py\n#17 CACHED\n\n#18 [ 8/20] RUN python -m venv /home/cc/EnvGym/data/TabPFN/venv\n#18 CACHED\n\n#19 [14/20] RUN /home/cc/EnvGym/data/TabPFN/venv/bin/pip install pytest pytest-xdist psutil\n#19 CACHED\n\n#20 [15/20] RUN /home/cc/EnvGym/data/TabPFN/venv/bin/pip install     \"ruff==0.8.6\"     \"mypy==1.17.0\"     \"pre-commit\"     \"commitizen\"     \"types-pyyaml\"     \"types-psutil\"     \"pyright\"     \"onnx\"\n#20 CACHED\n\n#21 [17/20] RUN mkdir -p .gemini &&     echo 'code_review:\\n  pull_request_opened:\\n    summary: false' > .gemini/config.yaml\n#21 CACHED\n\n#22 [19/20] RUN chmod +x scripts/generate_dependencies.py\n#22 CACHED\n\n#23 [20/20] RUN echo 'export PATH=/home/cc/EnvGym/data/TabPFN/venv/bin:$PATH' >> ~/.bashrc\n#23 DONE 0.3s\n\n#24 exporting to image\n#24 exporting layers\n#24 exporting layers 6.5s done\n#24 writing image sha256:2bfbc18bb8dbd9a00d57a05206e6e83b5dc87a225a8fbc0561ed7836a7a64384 done\n#24 naming to docker.io/library/envgym_test_1756818428 done\n#24 DONE 6.5s\n"
  },
  "run": {
    "success": true,
    "stdout": "",
    "stderr": ""
  }
}