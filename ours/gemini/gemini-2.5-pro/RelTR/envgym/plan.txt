Based on the provided hardware information, here is the adjusted environment setup plan. The key modifications focus on adapting the process for a CPU-only Linux environment, optimizing for a high core count, and ensuring all paths and commands are compatible with the specified setup.

***

### **ADJUSTED ENVIRONMENT SETUP PLAN (CPU-ONLY)**

This plan has been modified to be compatible with a CPU-only `linux/amd64` environment. All GPU-specific dependencies and commands have been removed or replaced with their CPU equivalents.

**Note:** All paths are relative to the project root directory, which is assumed to be `/home/cc/EnvGym/data/RelTR`.

1.  **DOWNLOADS NEEDED:**
    *   **Source Code:**
        *   RelTR Git Repository: `https://github.com/yrcong/RelTR.git`
    *   **Software & Tools:**
        *   Git version control system.
        *   Conda (Anaconda or Miniconda distribution).
        *   A C/C++ compiler toolchain and `make` (e.g., `build-essential` on Debian/Ubuntu).
    *   **Python Dependencies (via Conda/pip):**
        *   `python=3.6`
        *   `pytorch=1.6.0`
        *   `torchvision=0.7.0`
        *   `cpuonly` (Specifier for CPU-only PyTorch build)
        *   `matplotlib`
        *   `scipy=1.5.2`
        *   `pycocotools` (from COCO API GitHub repository)
        *   `setuptools` (for compiling custom modules)
        *   `cython` (for compiling Cython extensions)
        *   `numpy` (required for Cython compilation headers)
    *   **Pre-trained Models:**
        *   Visual Genome Model: From `https://drive.google.com/file/d/1id6oD_iwiNDD6HyCn2ORgRTIKkPD3tUD/view`
        *   Open Images V6 Model: From `https://drive.google.com/file/d/1pcoUnR0XWsvM9lJZ5f93N5TKHkLdjtnb/view?usp=share_link`
    *   **Datasets:**
        *   **Visual Genome (VG):**
            *   Images (Part 1): `https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip`
            *   Images (Part 2): `https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip`
            *   Annotations (COCO-format): `https://drive.google.com/file/d/1aGwEu392DiECGdvwaYr-LgqGLmWhn8yD/view?usp=sharing`
        *   **OpenImages V6 (OI):**
            *   Original Annotations (`.csv` files): From the official OpenImages [website](https://storage.googleapis.com/openimages/web/download.html).
            *   Images: From the PySGG project [page](https://github.com/SHTUPLUS/PySGG/blob/main/DATASET.md).
            *   Pre-processed Annotations (Optional alternative): `https://drive.google.com/file/d/1kWeG3O071Bx17KI7oLbMdgGvE5xmyY8k/view?usp=share_link`

2.  **FILES TO CREATE:**
    *   **Directory Structure:** The following directory structure must be created within the project root (`/home/cc/EnvGym/data/RelTR/`).
        ```
        RelTR/
        ├── ckpt/        # Untracked by Git. For storing downloaded model checkpoints.
        ├── data/
        │   ├── vg/
        │   │   ├── images/
        │   │   ├── rel.json   # Untracked by Git. Generated from downloaded annotations.
        │   │   ├── test.json  # Untracked by Git. Generated from downloaded annotations.
        │   │   ├── train.json # Untracked by Git. Generated from downloaded annotations.
        │   │   └── val.json   # Untracked by Git. Generated from downloaded annotations.
        │   └── oi/
        │       ├── images/
        │       ├── rel.json   # Untracked by Git. Generated by process.py script.
        │       ├── test.json  # Untracked by Git. Generated by process.py script.
        │       ├── train.json # Untracked by Git. Generated by process.py script.
        │       └── val.json   # Untracked by Git. Generated by process.py script.
        ...
        ```
    *   **Model Checkpoints:** Downloaded model files need to be placed and renamed as follows in the `ckpt/` directory.
        *   `ckpt/checkpoint0149.pth` (Visual Genome pre-trained model)
        *   `ckpt/checkpoint0149_oi.pth` (Open Images V6 pre-trained model)

3.  **NECESSARY TEST CASES IN THE CODEBASE:**
    *   **Inference Functionality Test:**
        *   Execute the `inference.py` script on a sample image using the CPU.
        *   **Verification:** Check for the creation of an output image (e.g., `demo/vg1_pred.png`). The script should complete without errors. **Note:** Execution will be significantly slower than on a GPU.
    *   **Visual Genome Evaluation Test:**
        *   Run the evaluation command for the Visual Genome dataset on the CPU.
        *   **Verification:** The script should run through the test set and print evaluation metrics to the console without crashing. This confirms the VG data was prepared correctly.
    *   **Open Images V6 Evaluation Test:**
        *   Run the evaluation command for the Open Images V6 dataset on the CPU.
        *   **Verification:** The script should run through the test set and print evaluation metrics to the console without crashing. This confirms the OI data was prepared correctly.
    *   **CPU Training Initialization Test:**
        *   Launch the training script for either dataset in a single process on the CPU.
        *   **Verification:** The script should successfully initialize, load the data, and start the first training epoch. This confirms that the data paths and environment are correct for CPU-based training.

4.  **COMPLETE TODO LIST:**
    *   **Step 1: System Prerequisites Installation**
        *   Action: Install Git, Conda, and a C/C++ compiler toolchain (like `build-essential`, which includes `gcc` and `make`) if they are not already present.
        *   Verification: Run `git --version`, `conda --version`, `gcc --version`, and `make --version` in your terminal to confirm installation.

    *   **Step 2: Clone the RelTR Repository**
        *   Action: Open a terminal, navigate to `/home/cc/EnvGym/data`, and run the following commands:
            ```bash
            git clone https://github.com/yrcong/RelTR.git
            cd RelTR
            ```
        *   Verification: The `/home/cc/EnvGym/data/RelTR` directory is populated and it is your current working directory.

    *   **Step 3: Create and Configure Conda Environment**
        *   Action: Run the following commands to create the Conda environment and install all necessary CPU-only Python dependencies.
            ```bash
            # Create the environment
            conda create -n reltr python=3.6 -y
            # Activate the environment
            conda activate reltr
            # Install CPU-only PyTorch, Torchvision, and other core packages
            conda install pytorch==1.6.0 torchvision==0.7.0 cpuonly -c pytorch -y
            conda install matplotlib scipy=1.5.2 -y
            # Install packages for building custom extensions (including Cython)
            pip install cython numpy
            pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
            pip install --upgrade setuptools
            ```
        *   Verification: Run `python --version` to see `Python 3.6.x`. Run `conda list` to confirm that `pytorch` and `torchvision` are installed from the `cpuonly` build.

    *   **Step 4: Compile Helper Code**
        *   Action: From the project root directory (`/home/cc/EnvGym/data/RelTR/`), run the compilation script, leveraging all available CPU cores for a faster build.
            ```bash
            (cd models/ops && make -j$(nproc))
            ```
        *   Verification: The script should complete without any compilation errors. Check for the presence of new compiled files (e.g., `*.so`) inside `models/ops/build/`.

    *   **Step 5: Download and Place Pre-trained Models**
        *   Action: Create the `ckpt` directory. Download the two pre-trained models from the provided links and place them in the `ckpt` directory with the specified names.
            *   Create directory: `mkdir ckpt`
            *   Download VG model and save as `ckpt/checkpoint0149.pth`.
            *   Download OI model and save as `ckpt/checkpoint0149_oi.pth`.
        *   Verification: Run `ls ckpt/` and confirm both `.pth` files are present.

    *   **Step 6: Prepare Datasets**
        *   **Step 6a: Prepare Visual Genome (VG) Dataset**
            *   Action: Download the VG images (Part 1 and Part 2) and the annotations zip file.
            *   Action: Unzip both image archives and place all `.jpg` files together inside `data/vg/images/`.
            *   Action: Unzip the annotations archive into the `data/` directory. This should create the required `train.json`, `val.json`, `test.json`, and `rel.json` files inside `data/vg/`.
            *   Verification: Check that `data/vg/images` is populated with images and that the four `.json` annotation files are present in `data/vg/`.

        *   **Step 6b: Prepare Open Images V6 (OI) Dataset**
            *   Action: Download the original OI V6 annotations (`.csv` files) and images.
            *   Action: Open `data/process.py` and modify the paths at the top of the file to point to your downloaded OI images and annotations.
            *   Action: Run the processing script: `python data/process.py`.
            *   Action: Move the newly created `images` directory to `data/oi/images/` and move the generated `.json` files to `data/oi/`.
            *   Verification: Check that `data/oi/images` is populated and that the four `.json` annotation files are present in `data/oi/`.

    *   **Step 7: Run Inference Test**
        *   Action: Execute the inference script, explicitly telling it to use the CPU.
            ```bash
            python inference.py --img_path demo/vg1.jpg --resume ckpt/checkpoint0149.pth --device cpu
            ```
        *   Verification: The script should save a result image `demo/vg1_pred.png`. Check that this file has been created.

    *   **Step 8: Run Evaluation on Visual Genome**
        *   Action: Run the evaluation command for the VG dataset on the CPU.
            ```bash
            python main.py --dataset vg --img_folder data/vg/images/ --ann_path data/vg/ --eval --batch_size 1 --resume ckpt/checkpoint0149.pth --device cpu
            ```
        *   Verification: The script should start the evaluation process and output metrics upon completion.

    *   **Step 9: Run Evaluation on Open Images V6**
        *   Action: Run the evaluation command for the OI dataset on the CPU.
            ```bash
            python main.py --dataset oi --img_folder data/oi/images/ --ann_path data/oi/ --eval --batch_size 1 --resume ckpt/checkpoint0149_oi.pth --device cpu
            ```
        *   Verification: The script should start the evaluation process and output metrics upon completion.

    *   **Step 10: (Optional) Run Training Initialization Test**
        *   Action: Launch the training script in a single process on the CPU. **WARNING:** This is for verifying the setup only. Actual training on a CPU will be impractically slow.
            ```bash
            # For Visual Genome
            python main.py --dataset vg --img_folder data/vg/images/ --ann_path data/vg/ --batch_size 2 --output_dir ckpt --device cpu
            
            # For Open Images V6
            python main.py --dataset oi --img_folder data/oi/images/ --ann_path data/oi/ --batch_size 2 --output_dir ckpt --device cpu
            ```
        *   Verification: The training process should initialize and begin the first epoch. Monitor the console for progress.